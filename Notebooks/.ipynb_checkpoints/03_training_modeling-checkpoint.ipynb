{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = \"font-family: garamond; font-size: 50px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :royalblue; border-radius: 100px 100px; text-align:center \" >Movie Plot Summarizer</h1>\n",
    "<h1 style = \"font-family: garamond; font-size: 30px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :royalblue; border-radius: 100px 100px; text-align:center \" >Training and Modeling</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = \"font-family: garamond; font-size: 45px; font-style: normal; letter-spacing: 3px; background-color: #f6f5f5; color :royalblue; border-radius: 100px 100px; text-align:center \" >Table of Contents</h1>\n",
    "\n",
    "\n",
    "* [1. Data Loading](#1)\n",
    "* [2. Introduction](#2)\n",
    "* [3. Extractive Summarization](#3)\n",
    "    * [3.1 Spacy](#3.1)\n",
    "    * [3.2 Gensim TextRank](#3.2)\n",
    "    * [3.3 Sumy Luhn](#3.3)\n",
    "* [4. Abstractive Summarization](#4)\n",
    "    * [4.1 Walkthrough of Encoder-Decoder Seq2Seq model](#4.1)\n",
    "    * [4.2 T5](#4.2)\n",
    "    * [4.3 Bert](#4.3)\n",
    "* [5. Results](#5)\n",
    "    * [5.1 Subjective measures](#5.1)\n",
    "    * [5.2 Objective measures](#5.2)\n",
    "* [6. Summary](#6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from rouge import Rouge\\n\\nfrom keras.preprocessing.text import Tokenizer \\nfrom nltk.corpus import stopwords   \\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.callbacks import EarlyStopping\\nimport warnings\\nimport nltk\\nnltk.download(\\'stopwords\\')\\n\\n#for asbtract summarization\\nimport gensim\\nfrom gensim.models import Word2Vec\\nfrom gensim.scripts.glove2word2vec import glove2word2vec\\nimport scipy\\nimport torch\\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\\n\\n# pd.set_option(\"display.max_colwidth\", 200)\\n# warnings.filterwarnings(\"ignore\")'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import ast\n",
    "import statistics\n",
    "\n",
    "'''from rouge import Rouge\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from nltk.corpus import stopwords   \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#for asbtract summarization\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import scipy\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "\n",
    "# pd.set_option(\"display.max_colwidth\", 200)\n",
    "# warnings.filterwarnings(\"ignore\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '1'></a>\n",
    "<h3 style = \"font-family:garamond; font-size:35px; background-color: white; color : royalblue; border-radius: 100px 100px; text-align:left\">Data Loading</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/df_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Overview</th>\n",
       "      <th>cleaned_plot</th>\n",
       "      <th>cleaned_overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1902</td>\n",
       "      <td>Jack and the Beanstalk</td>\n",
       "      <td>The earliest known adaptation of the classic f...</td>\n",
       "      <td>Abbott and Costello's version of the famous fa...</td>\n",
       "      <td>the earliest known adaptation of the classic f...</td>\n",
       "      <td>sostok _START_ abbott and costello version of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1905</td>\n",
       "      <td>The Night Before Christmas</td>\n",
       "      <td>Scenes are introduced using lines of the poem....</td>\n",
       "      <td>A cartoon based on the works of Nikolay Gogol....</td>\n",
       "      <td>scenes are introduced using lines of the poem ...</td>\n",
       "      <td>sostok _START_ a cartoon based on the works of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1906</td>\n",
       "      <td>Dream of a Rarebit Fiend</td>\n",
       "      <td>The Rarebit Fiend gorges on Welsh rarebit at a...</td>\n",
       "      <td>Adapted from Winsor McCay's films and comics o...</td>\n",
       "      <td>the rarebit fiend gorges on welsh rarebit at r...</td>\n",
       "      <td>sostok _START_ adapted from winsor mccay films...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1908</td>\n",
       "      <td>A Christmas Carol</td>\n",
       "      <td>No prints of the first American film adaptatio...</td>\n",
       "      <td>Reginald Owen portrays Charles Dickens' holida...</td>\n",
       "      <td>no prints of the first american film adaptatio...</td>\n",
       "      <td>sostok _START_ reginald owen portrays charles ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1910</td>\n",
       "      <td>Ramona</td>\n",
       "      <td>Ramona chronicles the romance between Ramona (...</td>\n",
       "      <td>Half-Indian girl brought up in a wealthy house...</td>\n",
       "      <td>ramona chronicles the romance between ramona m...</td>\n",
       "      <td>sostok _START_ half-indian girl brought up in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>2016</td>\n",
       "      <td>The Age of Shadows</td>\n",
       "      <td>A Korean police captain named Lee Jung-chool (...</td>\n",
       "      <td>Movie follows the activities of the Heroic Cor...</td>\n",
       "      <td>a korean police captain named lee jung-chool s...</td>\n",
       "      <td>sostok _START_ movie follows the activities of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3023</th>\n",
       "      <td>2016</td>\n",
       "      <td>My Annoying Brother</td>\n",
       "      <td>National Judo athlete Doo-young (Do Kyung-soo)...</td>\n",
       "      <td>Doo-Sik (Jo Jung-suk) gets paroled from prison...</td>\n",
       "      <td>national judo athlete doo-young do kyung-soo d...</td>\n",
       "      <td>sostok _START_ doo-sik jo jung-suk gets parole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>2017</td>\n",
       "      <td>Fabricated City</td>\n",
       "      <td>In real life, Kwon Yoo (Ji Chang-wook) is unem...</td>\n",
       "      <td>In real life, Kwon Yoo is unemployed, but in t...</td>\n",
       "      <td>in real life kwon yoo ji chang-wook is unemplo...</td>\n",
       "      <td>sostok _START_ in real life kwon yoo is unempl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3025</th>\n",
       "      <td>2017</td>\n",
       "      <td>New Trial</td>\n",
       "      <td>The film tells the story of a man whose life w...</td>\n",
       "      <td>A taxi driver is found dead, and Hyun-woo, the...</td>\n",
       "      <td>the film tells the story of man whose life was...</td>\n",
       "      <td>sostok _START_ a taxi driver is found dead and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3026</th>\n",
       "      <td>2009</td>\n",
       "      <td>Recep İvedik 2</td>\n",
       "      <td>The grandmother of rude big man Recep İvedik (...</td>\n",
       "      <td>Recep İvedik'in güldüren, tartışmaları da düşü...</td>\n",
       "      <td>the grandmother of rude big man recep i̇vedik ...</td>\n",
       "      <td>sostok _START_ recep i̇vedik in güldüren tartı...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3027 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Release Year                       Title  \\\n",
       "0             1902      Jack and the Beanstalk   \n",
       "1             1905  The Night Before Christmas   \n",
       "2             1906    Dream of a Rarebit Fiend   \n",
       "3             1908           A Christmas Carol   \n",
       "4             1910                      Ramona   \n",
       "...            ...                         ...   \n",
       "3022          2016          The Age of Shadows   \n",
       "3023          2016         My Annoying Brother   \n",
       "3024          2017             Fabricated City   \n",
       "3025          2017                   New Trial   \n",
       "3026          2009              Recep İvedik 2   \n",
       "\n",
       "                                                   Plot  \\\n",
       "0     The earliest known adaptation of the classic f...   \n",
       "1     Scenes are introduced using lines of the poem....   \n",
       "2     The Rarebit Fiend gorges on Welsh rarebit at a...   \n",
       "3     No prints of the first American film adaptatio...   \n",
       "4     Ramona chronicles the romance between Ramona (...   \n",
       "...                                                 ...   \n",
       "3022  A Korean police captain named Lee Jung-chool (...   \n",
       "3023  National Judo athlete Doo-young (Do Kyung-soo)...   \n",
       "3024  In real life, Kwon Yoo (Ji Chang-wook) is unem...   \n",
       "3025  The film tells the story of a man whose life w...   \n",
       "3026  The grandmother of rude big man Recep İvedik (...   \n",
       "\n",
       "                                               Overview  \\\n",
       "0     Abbott and Costello's version of the famous fa...   \n",
       "1     A cartoon based on the works of Nikolay Gogol....   \n",
       "2     Adapted from Winsor McCay's films and comics o...   \n",
       "3     Reginald Owen portrays Charles Dickens' holida...   \n",
       "4     Half-Indian girl brought up in a wealthy house...   \n",
       "...                                                 ...   \n",
       "3022  Movie follows the activities of the Heroic Cor...   \n",
       "3023  Doo-Sik (Jo Jung-suk) gets paroled from prison...   \n",
       "3024  In real life, Kwon Yoo is unemployed, but in t...   \n",
       "3025  A taxi driver is found dead, and Hyun-woo, the...   \n",
       "3026  Recep İvedik'in güldüren, tartışmaları da düşü...   \n",
       "\n",
       "                                           cleaned_plot  \\\n",
       "0     the earliest known adaptation of the classic f...   \n",
       "1     scenes are introduced using lines of the poem ...   \n",
       "2     the rarebit fiend gorges on welsh rarebit at r...   \n",
       "3     no prints of the first american film adaptatio...   \n",
       "4     ramona chronicles the romance between ramona m...   \n",
       "...                                                 ...   \n",
       "3022  a korean police captain named lee jung-chool s...   \n",
       "3023  national judo athlete doo-young do kyung-soo d...   \n",
       "3024  in real life kwon yoo ji chang-wook is unemplo...   \n",
       "3025  the film tells the story of man whose life was...   \n",
       "3026  the grandmother of rude big man recep i̇vedik ...   \n",
       "\n",
       "                                       cleaned_overview  \n",
       "0     sostok _START_ abbott and costello version of ...  \n",
       "1     sostok _START_ a cartoon based on the works of...  \n",
       "2     sostok _START_ adapted from winsor mccay films...  \n",
       "3     sostok _START_ reginald owen portrays charles ...  \n",
       "4     sostok _START_ half-indian girl brought up in ...  \n",
       "...                                                 ...  \n",
       "3022  sostok _START_ movie follows the activities of...  \n",
       "3023  sostok _START_ doo-sik jo jung-suk gets parole...  \n",
       "3024  sostok _START_ in real life kwon yoo is unempl...  \n",
       "3025  sostok _START_ a taxi driver is found dead and...  \n",
       "3026  sostok _START_ recep i̇vedik in güldüren tartı...  \n",
       "\n",
       "[3027 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youtube video on extractive summarization\n",
    "\n",
    "- Text cleaning\n",
    "- Sentence Tokenization\n",
    "- Word Tokenization\n",
    "- Word-frequency table\n",
    "- Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '2'></a>\n",
    "<h3 style = \"font-family:garamond; font-size:35px; background-color: white; color : royalblue; border-radius: 100px 100px; text-align:left\">Introduction to Text Summarization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text summarization is the method of extracting summaries from original text without losing any vital information. It has many applications and, much like NLP in general, has seen much improvement over the past few years. In this notebook, we will use our cleaned movie dataset to explore the more common approaches to text summarization and compare which is best. We will use some objective criteria like cosine similarity scores and rouge scores to compare the various methods, but at the time of this writing, nothing evaluates the results of summarization better than humans. Thus, we cannot avoid invoking our own subjective and flawed evaluations.\n",
    "\n",
    "- The first thing to know is that text summarization methods can be grouped into two categories: Extractive and Abstractive methods.\n",
    "- Extractive summarization seeks to identify the most significant sentences of the text and use them to create a summary. Thus, the summary contains the exact sentences from the original text.\n",
    "- Abstractive summarization seeks to identify the important sections and interpret the context in order to generate sentences for the summary. This method is far more advanced and has much more room for improvement over extractive methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '3'></a>\n",
    "<h3 style = \"font-family:garamond; font-size:35px; background-color: white; color : royalblue; border-radius: 100px 100px; text-align:left\">Extractive Models</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time we are going to use a selection of 10 movies to compare the generated summaries from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Overview</th>\n",
       "      <th>cleaned_plot</th>\n",
       "      <th>cleaned_overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1998</td>\n",
       "      <td>Get Real</td>\n",
       "      <td>Steven Carter (Ben Silverstone) is a 16-year-o...</td>\n",
       "      <td>A tenderly romantic coming-of-age story as two...</td>\n",
       "      <td>steven carter ben silverstone is 16-year-old m...</td>\n",
       "      <td>sostok _START_ a tenderly romantic coming-of-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999</td>\n",
       "      <td>A Walk on the Moon</td>\n",
       "      <td>Pearl Kantrowitz (Diane Lane) and her husband ...</td>\n",
       "      <td>The world of a young housewife is turned upsid...</td>\n",
       "      <td>pearl kantrowitz diane lane and her husband ma...</td>\n",
       "      <td>sostok _START_ the world of young housewife is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988</td>\n",
       "      <td>The Good Mother</td>\n",
       "      <td>Anna Dunlap (Keaton), is a piano teacher who w...</td>\n",
       "      <td>After finding a sexually liberated boyfriend, ...</td>\n",
       "      <td>anna dunlap keaton is piano teacher who works ...</td>\n",
       "      <td>sostok _START_ after finding sexually liberate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997</td>\n",
       "      <td>Henry Fool</td>\n",
       "      <td>Socially inept garbage-man Simon Grim is befri...</td>\n",
       "      <td>An egocentric bum transforms the lives of a sh...</td>\n",
       "      <td>socially inept garbage-man simon grim is befri...</td>\n",
       "      <td>sostok _START_ an egocentric bum transforms th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>Crazy/Beautiful</td>\n",
       "      <td>Nicole Oakley, the out-of-control daughter of ...</td>\n",
       "      <td>At Pacific Palisades High, a poor Latino falls...</td>\n",
       "      <td>nicole oakley the out-of-control daughter of c...</td>\n",
       "      <td>sostok _START_ at pacific palisades high poor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2007</td>\n",
       "      <td>The Savages</td>\n",
       "      <td>After drifting apart emotionally over the year...</td>\n",
       "      <td>A sister and brother face the realities of fam...</td>\n",
       "      <td>after drifting apart emotionally over the year...</td>\n",
       "      <td>sostok _START_ a sister and brother face the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1990</td>\n",
       "      <td>After Dark, My Sweet</td>\n",
       "      <td>Ex-boxer Kevin \"Kid\" Collins is a drifter and ...</td>\n",
       "      <td>The intriguing relationship between three desp...</td>\n",
       "      <td>ex-boxer kevin kid collins is drifter and an e...</td>\n",
       "      <td>sostok _START_ the intriguing relationship bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012</td>\n",
       "      <td>Broken</td>\n",
       "      <td>Eleven-year-old diabetic Emily 'Skunk' Cunning...</td>\n",
       "      <td>After dating a wonderful man, Hope comes back ...</td>\n",
       "      <td>eleven-year-old diabetic emily skunk cunningha...</td>\n",
       "      <td>sostok _START_ after dating wonderful man hope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>Mental</td>\n",
       "      <td>An underworld mafia steals valuable mineral re...</td>\n",
       "      <td>A charismatic, crazy hothead transforms a fami...</td>\n",
       "      <td>an underworld mafia steals valuable mineral re...</td>\n",
       "      <td>sostok _START_ a charismatic crazy hothead tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1989</td>\n",
       "      <td>She's Out of Control</td>\n",
       "      <td>Widower Doug Simpson (Danza) is a radio manage...</td>\n",
       "      <td>A Los Angeles radio-station manager's girlfrie...</td>\n",
       "      <td>widower doug simpson danza is radio manager fr...</td>\n",
       "      <td>sostok _START_ a los angeles radio-station man...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Release Year                 Title  \\\n",
       "0          1998              Get Real   \n",
       "1          1999    A Walk on the Moon   \n",
       "2          1988       The Good Mother   \n",
       "3          1997            Henry Fool   \n",
       "4          2001       Crazy/Beautiful   \n",
       "5          2007           The Savages   \n",
       "6          1990  After Dark, My Sweet   \n",
       "7          2012                Broken   \n",
       "8          2016                Mental   \n",
       "9          1989  She's Out of Control   \n",
       "\n",
       "                                                Plot  \\\n",
       "0  Steven Carter (Ben Silverstone) is a 16-year-o...   \n",
       "1  Pearl Kantrowitz (Diane Lane) and her husband ...   \n",
       "2  Anna Dunlap (Keaton), is a piano teacher who w...   \n",
       "3  Socially inept garbage-man Simon Grim is befri...   \n",
       "4  Nicole Oakley, the out-of-control daughter of ...   \n",
       "5  After drifting apart emotionally over the year...   \n",
       "6  Ex-boxer Kevin \"Kid\" Collins is a drifter and ...   \n",
       "7  Eleven-year-old diabetic Emily 'Skunk' Cunning...   \n",
       "8  An underworld mafia steals valuable mineral re...   \n",
       "9  Widower Doug Simpson (Danza) is a radio manage...   \n",
       "\n",
       "                                            Overview  \\\n",
       "0  A tenderly romantic coming-of-age story as two...   \n",
       "1  The world of a young housewife is turned upsid...   \n",
       "2  After finding a sexually liberated boyfriend, ...   \n",
       "3  An egocentric bum transforms the lives of a sh...   \n",
       "4  At Pacific Palisades High, a poor Latino falls...   \n",
       "5  A sister and brother face the realities of fam...   \n",
       "6  The intriguing relationship between three desp...   \n",
       "7  After dating a wonderful man, Hope comes back ...   \n",
       "8  A charismatic, crazy hothead transforms a fami...   \n",
       "9  A Los Angeles radio-station manager's girlfrie...   \n",
       "\n",
       "                                        cleaned_plot  \\\n",
       "0  steven carter ben silverstone is 16-year-old m...   \n",
       "1  pearl kantrowitz diane lane and her husband ma...   \n",
       "2  anna dunlap keaton is piano teacher who works ...   \n",
       "3  socially inept garbage-man simon grim is befri...   \n",
       "4  nicole oakley the out-of-control daughter of c...   \n",
       "5  after drifting apart emotionally over the year...   \n",
       "6  ex-boxer kevin kid collins is drifter and an e...   \n",
       "7  eleven-year-old diabetic emily skunk cunningha...   \n",
       "8  an underworld mafia steals valuable mineral re...   \n",
       "9  widower doug simpson danza is radio manager fr...   \n",
       "\n",
       "                                    cleaned_overview  \n",
       "0  sostok _START_ a tenderly romantic coming-of-a...  \n",
       "1  sostok _START_ the world of young housewife is...  \n",
       "2  sostok _START_ after finding sexually liberate...  \n",
       "3  sostok _START_ an egocentric bum transforms th...  \n",
       "4  sostok _START_ at pacific palisades high poor ...  \n",
       "5  sostok _START_ a sister and brother face the r...  \n",
       "6  sostok _START_ the intriguing relationship bet...  \n",
       "7  sostok _START_ after dating wonderful man hope...  \n",
       "8  sostok _START_ a charismatic crazy hothead tra...  \n",
       "9  sostok _START_ a los angeles radio-station man...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_10 = df.loc[(df['Overview'].str.split().str.len() <= 30) & (df['Release Year'] >= 1980)].sample(n=10)\n",
    "df_10 = df_10.reset_index(drop=True)\n",
    "df_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '3.1'></a>\n",
    "<h3 style = \"font-family:garamond; font-size:30px; background-color: white; color : royalblue; border-radius: 100px 100px; text-align:left\">SpaCy</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is a free, open-source natural language processing library. It provide a fast and accurate snyntactic analysis, named entity recognition and ready access to word vectors. It also offers tokenization, sentence boundary detection, POS tagging, snytactic parsing, integrated word vectors, and alignment into the original string with high accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''!python -m spacy download en_core_web_sm'''\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the English model into spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these extractive models are not learning from the whole dataset, let's select 10 random movies to use for comparision of models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will come back to our 10 movies. For now we will demonstrate this process on 1 random film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Overview</th>\n",
       "      <th>cleaned_plot</th>\n",
       "      <th>cleaned_overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>1997</td>\n",
       "      <td>The Brave</td>\n",
       "      <td>The film concerns a Native American man named ...</td>\n",
       "      <td>A down-on-his-luck American Indian recently re...</td>\n",
       "      <td>the film concerns native american man named ra...</td>\n",
       "      <td>sostok _START_ a down-on-his-luck american ind...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Release Year      Title  \\\n",
       "1830          1997  The Brave   \n",
       "\n",
       "                                                   Plot  \\\n",
       "1830  The film concerns a Native American man named ...   \n",
       "\n",
       "                                               Overview  \\\n",
       "1830  A down-on-his-luck American Indian recently re...   \n",
       "\n",
       "                                           cleaned_plot  \\\n",
       "1830  the film concerns native american man named ra...   \n",
       "\n",
       "                                       cleaned_overview  \n",
       "1830  sostok _START_ a down-on-his-luck american ind...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_samp = df.sample()\n",
    "single_samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the plot only and pass the string into the nlp function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = single_samp['Plot']\n",
    "doc = doc.reset_index()\n",
    "doc = doc.iloc[0]['Plot']\n",
    "doc = nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering Tokens: We need to create 2 lists for parts-of-speech and stop words to validate each token followed by filtering of the necessary tokens and saving them in the keywords list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = []\n",
    "stopwords = list(STOP_WORDS)\n",
    "pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "for token in doc:\n",
    "    if(token.text in stopwords or token.text in punctuation):\n",
    "        continue\n",
    "    if(token.pos_ in pos_tag):\n",
    "        keyword.append(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets calculate the frequency of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_word = Counter(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "for word in freq_word.keys():  \n",
    "    freq_word[word] = (freq_word[word]/max_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what this list should look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Raphael', 1.0),\n",
       " ('film', 0.75),\n",
       " ('wife', 0.5),\n",
       " ('children', 0.5),\n",
       " ('family', 0.5)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_word.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighing sentences: Each sentence will be weighed based on the frequency of the token present in each sentence. sent_strength is a python dictionary where the keys are the sentences and the string doc and the values are the weight of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_strength={}\n",
    "for sent in doc.sents:\n",
    "    for word in sent:\n",
    "        if word.text in freq_word.keys():\n",
    "            if sent in sent_strength.keys():\n",
    "                sent_strength[sent]+=freq_word[word.text]\n",
    "            else:\n",
    "                sent_strength[sent]=freq_word[word.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what this looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{The film concerns a Native American man named Raphael who lives with his wife and two children in a remote community near a rubbish dump selling whatever he can to make a living.: 5.75,\n",
       " Raphael, seeing the hopelessness of his situation and his inability to provide for his family, agrees to star in a snuff film for a large sum of money that he hopes will give his family a chance for a better life.: 7.0,\n",
       " \n",
       " Having been given the money in advance, Raphael is given a week to live and then return to be tortured and killed in front of the camera.: 4.75,\n",
       " The film follows Raphael's transformation with his relationship with his wife and children over the course of his final week of life and his own personal anguish with his fate.: 5.75}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, nlargest is used to summarize the string with 3 arguments: number of sentences to extract (we will do 5 sentences here); the list/tuple/dictionary to iterate over; the condition that needs to be satisfied. The resulting summarized_sentences is a list of sentences of spaCy.span type, which will be converted to a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raphael, seeing the hopelessness of his situation and his inability to provide for his family, agrees to star in a snuff film for a large sum of money that he hopes will give his family a chance for a better life. The film concerns a Native American man named Raphael who lives with his wife and two children in a remote community near a rubbish dump selling whatever he can to make a living. The film follows Raphael's transformation with his relationship with his wife and children over the course of his final week of life and his own personal anguish with his fate. \r\n",
      "Having been given the money in advance, Raphael is given a week to live and then return to be tortured and killed in front of the camera.\n"
     ]
    }
   ],
   "source": [
    "summarized_sentences = nlargest(8, sent_strength, key=sent_strength.get)\n",
    "final_sentences = [ w.text for w in summarized_sentences ]\n",
    "summary = ' '.join(final_sentences)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, back to our 10 random movies to use for comparison. Let's take everything we just did and make it into a function that we can use in a for loop for our 10 random movies dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_extract(df_plot):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(df_plot)\n",
    "    keyword = []\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "    for token in doc:\n",
    "        if(token.text in stopwords or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            keyword.append(token.text)\n",
    "    freq_word = Counter(keyword)\n",
    "    max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "    for word in freq_word.keys():  \n",
    "        freq_word[word] = (freq_word[word]/max_freq)\n",
    "    sent_strength={}\n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.text in freq_word.keys():\n",
    "                if sent in sent_strength.keys():\n",
    "                    sent_strength[sent]+=freq_word[word.text]\n",
    "                else:\n",
    "                    sent_strength[sent]=freq_word[word.text]\n",
    "    summarized_sentences = nlargest(3, sent_strength, key=sent_strength.get)\n",
    "    final_sentences = [ w.text for w in summarized_sentences ]\n",
    "    summary = ' '.join(final_sentences)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_generated_overview = []\n",
    "for i in df_10['Plot']:\n",
    "    spacy_generated_overview.append(spacy_extract(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Overview:  \r\n",
      "Word around the school spreads about someone being gay in the school, and John fears that Steven has been telling people. He is surprised to find the school jock, John Dixon (Brad Gorton) also cruising, but John denies that he is gay. \r\n",
      "At a school dance, Steven gains a friend after he comforts Jessica (Stacy Hart), after an argument with a boyfriend, who is also his bully, Kevin (Tim Harris). \n",
      "\n",
      "Generated Overview:  The movie begins with the couple and their family including their teenage daughter Alison (Anna Paquin) and young son Danny (Bobby Boriello) and Marty's mother Lillian (Tovah Feldshuh) going to their summer camp retreat, Dr. Fogler's Bungalows, which they attend each summer. Pearl Kantrowitz (Diane Lane) and her husband Marty (Liev Schreiber) are a lower middle class Jewish couple in New York City, where Marty is a television repairman. The final scene shows Pearl and Marty dancing together, first to Dean Martin's \"When You're Smiling\" and then to Jimi Hendrix's \"Purple Haze\", after Marty changes the station. \n",
      "\n",
      "Generated Overview:  Anna Dunlap (Keaton), is a piano teacher who works part-time at a college laboratory and a recent divorcee who has custody of a six-year-old daughter, Molly (Asia Viera). Worse, he accuses Leo of sexually molesting Molly and sues Anna for custody of their daughter. She meets and falls in love with Leo (Neeson), an Irish sculptor who helps her to find true passion and fulfillment. \n",
      "\n",
      "Generated Overview:  \r\n",
      "As Simon begins an ascent to the dizzying heights of Nobel Prize-winning poet, Henry sinks to a life of drinking in low-life bars as his own attempts at fame result in rejection, even by Simon's publisher who once employed Henry. Simon struggles to get his work recognized, and it is often dismissed as pornographic and scatological, but Henry continues to push and inspire Simon to get the poem published. Henry's hedonistic antics cause all manner of turns in the lives of Simon's family, not least of which is impregnating Fay, Simon's sister. \n",
      "\n",
      "Generated Overview:  Carlos is applying to the U.S. Naval Academy and Nicole's father suggests Carlos talk to him about gaining his Congressional sponsorship to the Academy. During their meeting, Nicole's father tells Carlos that he needs to end his relationship with Nicole, or she will destroy his life. \r\n",
      "As a result of this incident, Nicole's father and stepmother decide that she needs to go to a boarding school far away from home; Carlos rescues her and they run away together. \n",
      "\n",
      "Generated Overview:  After drifting apart emotionally over the years, two single siblings — Wendy (Linney) and Jon (Hoffman) — band together to care for their estranged, elderly father, Lenny (Philip Bosco), who is rapidly slipping into dementia. The film closes with Wendy running with her lover's dog alive, running with the aid of a wheeled hip cast, suggesting a mode of flawed yet persevering life for both siblings. Wendy and Jon first travel to Sun City, Arizona to attend the funeral of their father's girlfriend of 20 years. \n",
      "\n",
      "Generated Overview:  \r\n",
      "Reluctant in the beginning, Collie tries to leave and encounters Doc Goldman, who immediately can tell the young man needs to be under medical observation. Calling himself an ex-cop, he has long been hatching a scheme to kidnap a rich man's child and needs somebody like Collie to help carry it out. \r\n",
      "Resenting this interference, Collie is persuaded by Uncle Bud to execute the kidnap plan. \n",
      "\n",
      "Generated Overview:  \r\n",
      "Skunk is shocked when Rick gets beaten up by Mr. Oswald (Rory Kinnear), another neighbour: one of Mr. Oswald's daughters has accused Rick of rape. Eleven-year-old diabetic Emily 'Skunk' Cunningham (Eloise Laurence) lives with her father Archie (Tim Roth), her elder brother Jed (Bill Milner), and au pair Kasia (Zana Marjanović) in a typical British suburb. Rick does not let Skunk leave and does not realise when Skunk, deprived of her medical supply, falls into a hypoglycemic coma. \n",
      "\n",
      "Generated Overview:  An unknown, mysterious but affectionate and intimidating man comes up to help, The story revolves around revealing the true identity of him and his link to the Underworld.[3]\r\n",
      "Khan portrays the role of a generous businessman. Simi (Nusrat Imrose Tisha); a news reporter finds out and exposes the whole criminal chain to the media, despite all threats from them. Simi, portrayed by Tisha, is his love interest, and a crime news reporter. \n",
      "\n",
      "Generated Overview:  When his obsession with Katie and her boyfriends reaches extreme limits, Janet suggests that Doug needs psychiatric help and he seeks out an expert who gives him advice that goes wrong whenever it is applied. At the end of the film, Katie takes a class trip to Europe and reunites with Richard again – at which point Bonnie, her younger tomboy sister, begins her own dating spree. When Doug leaves on a business trip, Katie transforms herself into a beauty with help from her father's girlfriend Janet Pearson (Hicks). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in spacy_generated_overview:\n",
    "    print('Generated Overview: ', i, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '3.2'></a>\n",
    "<h3 style = \"font-family:garamond; font-size:30px; background-color: white; color : royalblue; border-radius: 100px 100px; text-align:left\">Gensim TextRank</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim has a summarization process based on TextRank Algorithm. TextRank algorithm is based on the concept that words that occur more frequently are significant, and thus the sentences containing these more frequent words are more important. TextRank algorithm assignes a score to each sentences and the top ranked sentences are used in the summary. Among the extractive methods, TextRank from Gensim seems to be the best currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6328/4286757646.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim's TextRank is much easier to use. We have the optional parameters of ratio (of summarized to original text, represent from 0-1) and word_count (the number of words to use in the summary). We can only use one, so we will use ratio set at .2 (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_samp_plot = single_samp.reset_index()\n",
    "single_samp_plot = single_samp.iloc[0]['Plot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sum = summarize(single_samp, ratio=0.2)\n",
    "print(short_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_generated_overview = []\n",
    "for i in df_10['Plot']:\n",
    "    gensim_generated_overview.append(summarize(i, ratio=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in gensim_generated_overview:\n",
    "    print('Generated Overview: ', i, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '3.3'></a>\n",
    "<h3 style = \"font-family:garamond; font-size:30px; background-color: white; color : royalblue; border-radius: 100px 100px; text-align:left\">From sumy, Luhn</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful when both low frequency words and high frequency words are not significant. (based on TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install sumy'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!pip install sumy'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to initialize the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=PlaintextParser.from_string(single_samp,Tokenizer('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instantiate the summarizer with the text document and choose the number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "luhn_summarizer=LuhnSummarizer()\n",
    "luhn_summary=luhn_summarizer(parser.document,sentences_count=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<Sentence: Release Year      Title  \\ 1830          1997  The Brave>, <Sentence: Plot  \\ 1830  The film concerns a Native American man named ...>, <Sentence: Overview  \\ 1830  A down-on-his-luck American Indian recently re...>, <Sentence: cleaned_plot  \\ 1830  the film concerns native american man named ra...>, <Sentence: cleaned_overview 1830  sostok _START_ a down-on-his-luck american ind...>)\n"
     ]
    }
   ],
   "source": [
    "print(luhn_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run this on our 10 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumy_luhn_generated_overview = []\n",
    "for i in df_10['Plot']:\n",
    "    parser=PlaintextParser.from_string(i, Tokenizer('english'))\n",
    "    luhn_summarizer = LuhnSummarizer()\n",
    "    luhn_summary = luhn_summarizer(parser.document, sentences_count=6)\n",
    "    gen_sum = ''\n",
    "    for sen in luhn_summary:\n",
    "        gen_sum = gen_sum + str(sen) + ' '\n",
    "    sumy_luhn_generated_overview.append(gen_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Overview:  He is surprised to find the school jock, John Dixon (Brad Gorton) also cruising, but John denies that he is gay. At a school dance, Steven gains a friend after he comforts Jessica (Stacy Hart), after an argument with a boyfriend, who is also his bully, Kevin (Tim Harris). Word around the school spreads about someone being gay in the school, and John fears that Steven has been telling people. In order to maintain his status in the school, John beats up Steven in front of his friends. Steven announces in front of assembly that he is gay, and looks to John for support, but he does not. In the end, John apologizes for beating him up and says he loves him, but as he is too afraid to come out, Steven breaks up with him, wishing him happiness.  \n",
      "\n",
      "Generated Overview:  Meanwhile, Alison is neglected and she experiences her first period, her first date, and her first kiss as she enters a relationship with another boy at the camp, Ross Epstein. Marty's mother Lillian learns of the affair and tries to persuade Pearl to break it off. The affair continues when Marty cannot get up to visit on the weekend because of the traffic jam caused by the Woodstock festival, which is within walking distance of the bungalow colony. Pearl goes to the festival, and unbeknownst to her, Alison goes as well with Ross and her friends despite her mother previously forbidding it. Marty learns of the affair and confronts Pearl while Alison confronts her mother in an emotional scene. Pearl is forced to deal with her love of her family and her conflicting yearning for marital freedom.  \n",
      "\n",
      "Generated Overview:  Anna Dunlap (Keaton), is a piano teacher who works part-time at a college laboratory and a recent divorcee who has custody of a six-year-old daughter, Molly (Asia Viera). She meets and falls in love with Leo (Neeson), an Irish sculptor who helps her to find true passion and fulfillment. But her sexual liberation comes at a very high cost: her ex-husband Brian (James Naughton) questions her new lifestyle. Worse, he accuses Leo of sexually molesting Molly and sues Anna for custody of their daughter.  \n",
      "\n",
      "Generated Overview:  Henry opens the world of literature to Simon, and inspires him to write \"the great American poem.\" Simon struggles to get his work recognized, and it is often dismissed as pornographic and scatological, but Henry continues to push and inspire Simon to get the poem published. Henry carries around a bundle of notebooks that he refers to as his \"Confession,\" a work that details aspects of his mysterious past that he one day hopes to publish, when he and the world is ready for them. Henry's hedonistic antics cause all manner of turns in the lives of Simon's family, not least of which is impregnating Fay, Simon's sister. As Simon begins an ascent to the dizzying heights of Nobel Prize-winning poet, Henry sinks to a life of drinking in low-life bars as his own attempts at fame result in rejection, even by Simon's publisher who once employed Henry. The friends part ways and lose touch, until Henry’s criminal past catches up with him and he needs Simon’s help to flee the country.  \n",
      "\n",
      "Generated Overview:  Nicole's mother committed suicide when she was very young, and feels unwanted by her father, who is now married to another woman and has another young daughter with his new wife. During their meeting, Nicole's father tells Carlos that he needs to end his relationship with Nicole, or she will destroy his life. As a result of this incident, Nicole's father and stepmother decide that she needs to go to a boarding school far away from home; Carlos rescues her and they run away together. While they are away, Nicole realizes she is getting in the way of Carlos' dreams, so she decides to sober up. They go back home and she makes up with her father. In the end credits, we see that Carlos has become a pilot with the Navy.  \n",
      "\n",
      "Generated Overview:  They then move him to a nursing home in Buffalo, where Jon is a theater professor working on a book about Bertolt Brecht. It is implied that he was a physically and emotionally abusive father when Jon and Wendy were growing up and they cut him out of their lives. She is sleeping with an unattainable married man 13 years her senior and he cannot commit to a Polish woman who must return to Kraków after her visa expires. In the end, Wendy has broken up with her married lover, but has adopted his dog, which he had planned to put down. She is also seen working on the production of her play about their terrible childhood, while Jon is leaving for a conference in Poland where it is suggested he may reconnect with the woman he had let go. The film closes with Wendy running with her lover's dog alive, running with the aid of a wheeled hip cast, suggesting a mode of flawed yet persevering life for both siblings.  \n",
      "\n",
      "Generated Overview:  Ex-boxer Kevin \"Kid\" Collins is a drifter and an escapee from a mental hospital. In a desert town near Palm Springs he meets widow Fay Anderson who convinces him to help fix up the neglected estate her husband left. She nicknames him \"Collie\" and lets him sleep in a trailer out back, near her dying date palms. Calling himself an ex-cop, he has long been hatching a scheme to kidnap a rich man's child and needs somebody like Collie to help carry it out. Reluctant in the beginning, Collie tries to leave and encounters Doc Goldman, who immediately can tell the young man needs to be under medical observation. But things go wrong from the very beginning, including Collie snatching the wrong kid.  \n",
      "\n",
      "Generated Overview:  Skunk is shocked when Rick gets beaten up by Mr. Oswald (Rory Kinnear), another neighbour: one of Mr. Oswald's daughters has accused Rick of rape. Rick is put into a closed ward as he isolates himself more and more. Skunk's first boyfriend suddenly has to move to his aunt's, something he didn't dare tell her until the day before he leaves. When she enters the house, she finds that Rick has pushed his mother down the stairs, breaking her neck, and knocked out his father. Rick does not let Skunk leave and does not realise when Skunk, deprived of her medical supply, falls into a hypoglycemic coma. She wakes up in a hospital bed, with her father at her bedside.  \n",
      "\n",
      "Generated Overview:  Simi (Nusrat Imrose Tisha); a news reporter finds out and exposes the whole criminal chain to the media, despite all threats from them. An unknown, mysterious but affectionate and intimidating man comes up to help, The story revolves around revealing the true identity of him and his link to the Underworld. Simi, portrayed by Tisha, is his love interest, and a crime news reporter. Simi takes steps to fight the corrupt people, but soon realizes the influence of those men. Khan changes after a violent encounter in which his love interest, Simi was killed. The plot builds up around his quest to avenge the killing with aid of her research, in which she documents her encounter with her killers, who she planned to expose of corruption  \n",
      "\n",
      "Generated Overview:  Widower Doug Simpson (Danza) is a radio manager from California who lives with his two daughters, Katie (Dolenz) and Bonnie (Laura Mooney). She's been dating Richard, the boy next door, whom her father adores, since middle school. When Doug leaves on a business trip, Katie transforms herself into a beauty with help from her father's girlfriend Janet Pearson (Hicks). When his obsession with Katie and her boyfriends reaches extreme limits, Janet suggests that Doug needs psychiatric help and he seeks out an expert who gives him advice that goes wrong whenever it is applied. Throughout the latter half of the film, Katie has three boyfriends, two of whom she eventually stops dating. At the end of the film, Katie takes a class trip to Europe and reunites with Richard again – at which point Bonnie, her younger tomboy sister, begins her own dating spree.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in sumy_luhn_generated_overview:\n",
    "    print('Generated Overview: ', i, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '4'></a>\n",
    "<h3 style = \"font-family:garamond; font-size:35px; background-color: white; color : royalblue; border-radius: 100px 100px; text-align:left\">Abstractive Summarization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '4.1'></a>\n",
    "<h3 style = \"font-family:garamond; font-size:30px; background-color: white; color : royalblue; border-radius: 100px 100px; text-align:left\">Walkthrough of building Encoder-Decoder Seq2Seq recurrent neural network (RNN)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - RNNs usually have fixed size input and output vectors. This is obviously quite undesirable for machine translation. Seq2Seq models can help us solve this problem. How? Because the fixed size output vector generated by the encoder need not be the same size as the input vector give to the decoder. It can be passed in whole or can be connected ot the hidden units of the decoder unit at every time step. \n",
    " - For text summarization, our model will have an encoder that accepts the pre-processed Plot and Overview, trains the model to create an encoded representation, and sends it to a decoder which decodes the endcoded representation into a reliable summary. With more and more training, the model can be used to perform inference on new texts to generate new summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text summarization is a rapidly advancing area of NLP. With our cleaned dataset, we are going to explore some of the most common strategies of text summarization available and compare how they perform on our dataset, using both objective and subjective methods of comparison. This will be a practical guide on text summarization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(\n",
    "    np.array(df[\"cleaned_plot\"]),\n",
    "    np.array(df[\"cleaned_overview\"]),\n",
    "    test_size=0.1,\n",
    "    random_state=0,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text to get the vocab count \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Prepare a tokenizer on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COULD TRY THE OTHER ONE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary:  68.75295006136128\n"
     ]
    }
   ],
   "source": [
    "thresh = 5\n",
    "\n",
    "cnt = 0\n",
    "tot_cnt = 0\n",
    "\n",
    "for key, value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt = tot_cnt + 1\n",
    "    if value < thresh:\n",
    "        cnt = cnt + 1\n",
    "    \n",
    "print(\"% of rare words in vocabulary: \", (cnt / tot_cnt) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_plot_len = 375\n",
    "max_overview_len = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary in X = 9931\n"
     ]
    }
   ],
   "source": [
    "# Prepare a tokenizer, again -- by not considering the rare words\n",
    "x_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "# Convert text sequences to integer sequences \n",
    "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "# Pad zero upto maximum length\n",
    "x_tr = pad_sequences(x_tr_seq,  maxlen=max_plot_len, padding='post')\n",
    "x_val = pad_sequences(x_val_seq, maxlen=max_plot_len, padding='post')\n",
    "\n",
    "# Size of vocabulary (+1 for padding token)\n",
    "x_voc = x_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in X = {}\".format(x_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 81.22323664862245\n",
      "Size of vocabulary in Y = 2325\n"
     ]
    }
   ],
   "source": [
    "# Prepare a tokenizer on testing data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "thresh = 5\n",
    "\n",
    "cnt = 0\n",
    "tot_cnt = 0\n",
    "\n",
    "for key, value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt = tot_cnt + 1\n",
    "    if value < thresh:\n",
    "        cnt = cnt + 1\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt / tot_cnt) * 100)\n",
    "\n",
    "# Prepare a tokenizer, again -- by not considering the rare words\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "# Convert text sequences to integer sequences \n",
    "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "# Pad zero upto maximum length\n",
    "y_tr = pad_sequences(y_tr_seq, maxlen=max_overview_len, padding='post')\n",
    "y_val = pad_sequences(y_val_seq, maxlen=max_overview_len, padding='post')\n",
    "\n",
    "# Size of vocabulary (+1 for padding token)\n",
    "y_voc = y_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty Summaries, .i.e, which only have 'START' and 'END' tokens\n",
    "ind = []\n",
    "\n",
    "for i in range(len(y_tr)):\n",
    "    cnt = 0\n",
    "    for j in y_tr[i]:\n",
    "        if j != 0:\n",
    "            cnt = cnt + 1\n",
    "    if cnt == 2:\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr = np.delete(y_tr, ind, axis=0)\n",
    "x_tr = np.delete(x_tr, ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty Summaries, .i.e, which only have 'START' and 'END' tokens\n",
    "ind = []\n",
    "for i in range(len(y_val)):\n",
    "    cnt = 0\n",
    "    for j in y_val[i]:\n",
    "        if j != 0:\n",
    "            cnt = cnt + 1\n",
    "    if cnt == 2:\n",
    "        ind.append(i)\n",
    "\n",
    "y_val = np.delete(y_val, ind, axis=0)\n",
    "x_val = np.delete(x_val, ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, \\\n",
    "    Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 375)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 375, 200)     1986200     ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  [(None, 375, 300),   601200      ['embedding_2[0][0]']            \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  [(None, 375, 300),   721200      ['lstm_4[0][0]']                 \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, None, 200)    465000      ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)                  [(None, 375, 300),   721200      ['lstm_5[0][0]']                 \n",
      "                                 (None, 300),                                                     \n",
      "                                 (None, 300)]                                                     \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  [(None, None, 300),  601200      ['embedding_3[0][0]',            \n",
      "                                 (None, 300),                     'lstm_6[0][1]',                 \n",
      "                                 (None, 300)]                     'lstm_6[0][2]']                 \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, None, 2325)  699825      ['lstm_7[0][0]']                 \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,795,825\n",
      "Trainable params: 5,795,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 300\n",
    "embedding_dim = 200\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_plot_len, ))\n",
    "\n",
    "# Embedding layer\n",
    "enc_emb = Embedding(x_voc, embedding_dim,\n",
    "                    trainable=True)(encoder_inputs)\n",
    "\n",
    "# Encoder LSTM 1\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n",
    "                     return_state=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n",
    "\n",
    "# Encoder LSTM 2\n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n",
    "                     return_state=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# Encoder LSTM 3\n",
    "encoder_lstm3 = LSTM(latent_dim, return_state=True,\n",
    "                     return_sequences=True, dropout=0.4,\n",
    "                     recurrent_dropout=0.4)\n",
    "(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using encoder_states as the initial state\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "\n",
    "# Embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# Decoder LSTM\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
    "                    return_state=True, dropout=0.4,\n",
    "                    recurrent_dropout=0.2)\n",
    "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
    "    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will impliment an early stopping mechanism to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12092/1524082473.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = model.fit(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0my_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2860\u001b[0m     \u001b[1;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2861\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2862\u001b[1;33m       raise RuntimeError('You must compile your model before '\n\u001b[0m\u001b[0;32m   2863\u001b[0m                          \u001b[1;34m'training/testing. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2864\u001b[0m                          'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [x_tr, y_tr[:, :-1]],\n",
    "    y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:],\n",
    "    epochs=30,\n",
    "    callbacks=[es],\n",
    "    batch_size=64,\n",
    "    validation_data=([x_val, y_val[:, :-1]],\n",
    "                     y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:\n",
    "                     , 1:]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhdklEQVR4nO3deXTU9b3/8ec7CwmQsCXsEBJxARWMgLKEuoALaOvSWlsXbOG2tL9767W33l7E37GtbX9VW4/VHrX8uAq4VevP5aqttm4gsiooSBRUlATCIhBkSSBAyPv3x0wwhElmksxkMpPX45w5zOT7mW/enxPOez7zns983+buiIhI4kuJdwAiIhIdSugiIklCCV1EJEkooYuIJAkldBGRJJEWr1+cm5vr+fn58fr1IiIJaeXKlTvdvWeoY3FL6Pn5+axYsSJev15EJCGZWWlDx1RyERFJEkroIiJJQgldRCRJxK2GLiLSHIcPH6asrIyqqqp4hxJTmZmZDBgwgPT09Iifo4QuIgmlrKyM7Oxs8vPzMbN4hxMT7k55eTllZWUUFBRE/DyVXEQkoVRVVZGTk5O0yRzAzMjJyWnyuxAldBFJOMmczGs1Z44Jl9C/2FvFHS+vZfPuA/EORUSkTQmb0M0s08zeMbPVZvahmd0eYszlZvaBma0ysxVmNj424cKyz8t5aNEGzvn9fP71iZW8W7ILXdNdRFrL7t27efDBB5v8vEsuuYTdu3dHP6A6IlmhHwQmuPsZQCEwyczG1BvzBnCGuxcC04CHohlkXZcX9mfhf53PD75WwKJPd/LtWUu57P7FPP9+GYeqa2L1a0VEgIYT+pEjRxp93ssvv0y3bt1iFFVA2ITuARXBh+nBm9cbU+FfLZM71z8ebf27dWTm5KEsu3Uiv73idPYfquY//rqaorve5E9vfMrOioOx/PUi0o7dcsstfPbZZxQWFnLWWWdx/vnnc+211zJs2DAArrjiCkaOHMlpp53G7Nmzjz4vPz+fnTt3UlJSwtChQ/nhD3/IaaedxkUXXcSBA9EpIVsk5QozSwVWAicCD7j7jBBjrgTuAHoBl7r70hBjpgPTAfLy8kaWljZ4SYImqalx3l6/kzmLNvDWJzvokJbC5Wf0Y2pRAaf26xKV3yEibcPatWsZOnQoALe/9CEfbdkb1fOf2q8Lv/zGaQ0eLykp4etf/zrFxcUsWLCASy+9lOLi4qPbC3ft2kWPHj04cOAAZ511Fm+99RY5OTlHr19VUVHBiSeeyIoVKygsLOTqq6/msssu4/rrr290rrXMbKW7jwoVW0T70N39CFBoZt2A583sdHcvrjfm+eCxc4DfABeEOM9sYDbAqFGjoraKT0kxzj25J+ee3JP12yuYt2QDz67czP9bWcaYE3owraiAiUN7k5qS/J+Mi0jrOvvss4/ZK/6nP/2J559/HoBNmzbx6aefkpOTc8xzCgoKKCwsBGDkyJGUlJREJZYmfbHI3Xeb2QJgElDcwJiFZjbYzHLdfWcUYmySE3tl8dsrhvHzi4bw1LsbeWRJCdMfW0lej058b1w+V48aQHZm5N+8EpG2q7GVdGvp3Lnz0fsLFizg9ddfZ+nSpXTq1Inzzjsv5F7yjIyMo/dTU1OjVnKJZJdLz+DKHDPrSGDlva7emBMtuGnSzEYAHYDyqETYTF07pfOjcwez8L/O58HrRtArO4Pf/O0jxt7xJr968UNKdlbGMzwRSVDZ2dns27cv5LE9e/bQvXt3OnXqxLp161i2bFmrxhbJCr0v8Eiwjp4CPO3ufzOzHwO4+yzgW8ANZnYYOAB8x9vIXsK01BQuGdaXS4b15YOy3cxdXMITy0t5ZGkJE4f0YlpRAWMHJ/e3zkQkenJycigqKuL000+nY8eO9O7d++ixSZMmMWvWLIYPH84pp5zCmDH1NwTGVkQfisbCqFGjPF4NLrbvreLxZaU8vnwjuyoPMaRPNlOL8rm8sD+Z6alxiUlEIhPqg8Jk1dQPRRPum6LR0KtLJj+76BSW3DKB3181HIAZz65h3J1vcvc/P+aLvcl9FTcRSU7t+mqLmempXD1qIN8eOYCln5czd3EJDyxYz6y3PuPS4X2ZVlTAGQO7xTtMEZGItOuEXsvMGDc4l3GDcyktr+SRJaU8vWITL6zawshB3ZlalM+k0/qQltou39CISIJQQq9nUE5nfvGNU/mPC0/imZVlzFtSwk/+8j79umYyZWw+15w9kG6dOsQ7TBGR42jJ2YDszHSmFhXw5s3n8d83jCI/tzN3/WMdY+54g1ufX8P67aG3LYmIxItW6GGkphgXntqbC0/tzdqte5m7eAPPrCzjL8s3cs7JPZlalM+5J/UkRd9CFZE40wq9CYb27cLvrzqDpbdM4OYLT2bd1r1MnfsuF/zxLR5bVsr+Q9XxDlFEYqy5l88FuPfee9m/f3+UI/qKEnoz5GRlcOPEk1g0YwL3fqeQrIw0bvufYsb87g3ueHktZV/G7g8mIvHVlhO6Si4t0CEthSvO7M/lhf14b+OXzFlUwkOLNvDfb3/OpNP7MLWogFGDuutbqCJJpO7lcy+88EJ69erF008/zcGDB7nyyiu5/fbbqays5Oqrr6asrIwjR45w22238cUXX7BlyxbOP/98cnNzmT9/ftRjU0KPAjNj5KAejBzUg827D/Do0hKeemcTL6/ZxrD+XZk2Pp9Lh/WjQ5reEIlE1Su3wLY10T1nn2Ew+c4GD995550UFxezatUqXn31VZ555hneeecd3J3LLruMhQsXsmPHDvr168ff//53IHCNl65du3LPPfcwf/58cnNzoxtzkDJMlNU231g6c4Kab4gkuVdffZVXX32VM888kxEjRrBu3To+/fRThg0bxuuvv86MGTN4++236dq1a6vEoxV6jHTqkMb1YwZx7dl5R5tv3PPaJ9w/f72ab4hESyMr6dbg7sycOZMf/ehHxx1buXIlL7/8MjNnzuSiiy7iF7/4RczjUUKPMTXfEEkudS+fe/HFF3Pbbbdx3XXXkZWVxebNm0lPT6e6upoePXpw/fXXk5WVxbx58455bqxKLkrorUjNN0QSX93L506ePJlrr72WsWPHApCVlcXjjz/O+vXr+fnPf05KSgrp6en8+c9/BmD69OlMnjyZvn37xuRD0XZ5+dy2ovpIDa9+9AVzFm1gRemXdO6QyrdHDeT74/LJz+0c/gQi7ZAun9vCnqISG2q+ISLRpITeRgwf0I0/fqeQmZOHHG2+8fra5Wq+ISIR07bFNqah5htj73hDzTdEgtpIh8uYas4cI2kSnWlm75jZajP70MxuDzHmOjP7IHhbYmZnNDkSOUZt841Xbvoaf/nhaEbl9+CBBespuvNNbnrqfVZv2h3vEEXiIjMzk/Ly8qRO6u5OeXk5mZmZTXpe2A9FLVDA7ezuFWaWDiwCbnL3ZXXGjAPWuvuXZjYZ+JW7j27svPpQtOnqNt+oOFjNiLxuTBtfoOYb0q4cPnyYsrIyqqqS+91qZmYmAwYMID392J1vjX0o2qRdLmbWiUBC/1/uvryBMd2BYnfv39i5lNCbb1/V4aPNN0rL99O3ayY3qPmGSLvQ4oRuZqnASuBE4AF3n9HI2P8Ehrj7D0Icmw5MB8jLyxtZWloa2QwkpCM1zpvrtjN38QaWfFZOZnoK3xwxgGlF+ZzYKzve4YlIDERzhd4NeB640d2LQxw/H3gQGO/u5Y2dSyv06Fq7dS/zFpfw/KrNHKquUfMNkSQVtYQePNkvgUp3v7vez4cTSPaT3f2TcOdRQo+N8oqD/GX5Rh5bVsr2fQc5oWdnpo7L55sjBtA5Q7tURRJdixK6mfUEDrv7bjPrCLwK3OXuf6szJg94E7jB3ZdEEpQSemwdqq7h5TVbmbN4Ax+U7aFLZhrXnJ3HlLGDGNC9U7zDE5FmamlCHw48AqQS2Ob4tLv/2sx+DODus8zsIeBbQG1RvLqhX1hLCb11uHug+cbiEv5RvA13V/MNkQQW1ZJLtCiht766zTf2HDjMsP5dmVqUz9eHq/mGSKJQQpdj7D9UzXPvbWbu4g18tqOSntkZTBkziGtH55GblRHv8ESkEUroElJNjfP2+p3MXbyBBR/voENaippviLRxutqihBSu+cbUogIuUPMNkYShFbocY8/+w0ebb2zZU8XAHh35/rgCNd8QaSNUcpEmU/MNkbZJCV1apLb5xt8+2EJ1jTNxSC+mFhUwTs03RFqdErpExfa9VUebb+yqPMQpvbOZNl7NN0RakxK6RFXV4SO8uHoLcxZtYN22fXTvlM51owcxZewgendp2vWbRaRplNAlJtydZZ/vYs7iDby+9gtSzbh0eF+mFhVQOLBbvMMTSUratigxYWaMHZzD2ME5xzTfeGHVFjXfEIkDrdAlqtR8QyS2VHKRVnekxpm/bjtz6jXfmDoun5N6q/mGSHMpoUtc1W++8bWTcpk2vkDNN0SaQQld2oTyioM8+c5GHl2q5hsizaWELm3KoeoaXineysOLvmq+8d2z87hBzTdEwlJClzYpVPONi0/rw7Txar4h0hBtW5Q2ycwYOagHIwf1YPPuAzy2tJQn39nIK8Xb1HxDpBm0Qpc2Zf+hap5/fzNzFqn5hkgoLe0pmgksBDIIrOifcfdf1hszBJgLjAD+t7vfHS4oJXRpjJpviITW0pLLQWCCu1eYWTqwyMxecfdldcbsAv4duKLF0Yqg5hsizRG2OOkBFcGH6cGb1xuz3d3fBQ5HP0Rp707slcVvrxjGspkTmTl5CJt2HeBHj63kvLvn8/CiDeyr0n87EYiwhm5mqcBK4ETgAXef0cC4XwEVDZVczGw6MB0gLy9vZGlpaTPDlvastvnG3MUbeLdEzTekfYnatkUz6wY8D9zo7sUhjv+KRhJ6XaqhSzSo+Ya0N40l9CbtB3P33cACYFLLwxJpueEDuvHH7xSyeMYEbjz/RN7fuJvrHlrOpHvf5q/vbqTq8JF4hyjSasImdDPrGVyZY2YdgQuAdTGOS6RJenXJ5GcXncLiWybw+6uGYwYznl3D2Dve4O5/fswXe6viHaJIzEWybXE48AiQSuAF4Gl3/7WZ/RjA3WeZWR9gBdAFqAEqgFPdfW9D51XJRWJJzTckWemr/9Ku1W2+UXGwWs03JKEpoYug5huSHJTQRepQ8w1JZEroIg1Q8w1JNEroImGo+YYkCiV0kQjVb76RnZnGNWq+IW2IErpIE6n5hrRVanAh0kR1m29s2X2AR+s03zi9fxemFRVw6fC+ZKSlxjtUkaO0QheJUKjmG9ePHsR1Y9R8Q1qPSi4iUXRc843UFC4r7MfUonxO69c13uFJklPJRSSK6jffeGRJCc+sLOMZNd+QONMKXSQK9uw/zFPvBrY9bt59gIE9OvK9sflcfdZAumSmxzs8SSIquYi0EjXfkFhTQheJgzVle5i7eAMvqfmGRJESukgcbd9bxePLSnli+UbKKw9xSu9sphblc8WZ/clM17ZHaRoldJE2oOrwEV5cvYU5izawbts+undK57rRg5gydhC9u2TGOzxJEEroIm2Imm9IS2jbokgbYmaMHZzD2ME5bCzfz7wlJTy9YhMvrNrCiLxuTC0qYNLpfUhX8w1pokha0GUCC4EMAi8Az7j7L+uNMeA+4BJgP/B9d3+vsfNqhS7yFTXfkEi1qOQSTNad3b3CzNKBRcBN7r6szphLgBsJJPTRwH3uPrqx8yqhixxPzTcknBaVXDyQ8SuCD9ODt/qvApcDjwbHLjOzbmbW1923tiBukXYnNcW44NTeXHBqb9Zt28vcRYFvof5l+UY135CwIirSmVmqma0CtgOvufvyekP6A5vqPC4L/kxEmmlIny7cddVwlt4ygf+86GQ+3raPqXPf5YJ73uKxpSVUHqyOd4jSxkSU0N39iLsXAgOAs83s9HpDQi0XjqvlmNl0M1thZit27NjR5GBF2qOcrAx+MuEkFs2YwH3fLSQ7M43bXviQMXe8we9eXkvZl/vjHaK0EU3etmhmvwQq3f3uOj/7v8ACd38y+Phj4LzGSi6qoYs0T6D5xm7mLN6g5hvtUItq6GbWEzjs7rvNrCNwAXBXvWEvAj8xs6cIfCi6R/VzkdgINN/ozshB3dV8Q44RyS6X4cAjQCqBEs3T7v5rM/sxgLvPCu6EuR+YRGDb4lR3b3T5rRW6SPTUNt+Yu7iE9dsr1HwjiemboiLthLvz9qc7maPmG0lL3xQVaSfMjHNO7sk5ar7RLmmFLpLk9uw/zF9XbOSRJWq+kQxUchERqo/U8NpHXzBHzTcSmhK6iBxDzTcSlxK6iISk5huJRwldRBpVdfgIL63ewpzFJazdulfNN9owJXQRiUht8425izfwmppvtEnatigiEVHzjcSmFbqINErNN9oWlVxEpMXUfKNtUEIXkaiqbb7x/KrNHKquCTTfKCrg3JPVfCPWlNBFJCbKKw7y5DsbeXRpKdv3HeSE3M5MLcrnmyMG0DlDH9HFghK6iMTUoeoaXineypxFG1hdtofszDSuOTuPG8YOYkD3TvEOL6kooYtIq1DzjdjTtkURaRVqvhFfWqGLSEzVb76Rm5XBlDFqvtFcKrmISNyp+UZ0qOQiInHXWPON0QU9mDZezTdaKpKeogOBR4E+QA0w293vqzemOzAHGAxUAdPcvbix82qFLiJqvtF0LSq5mFlfoK+7v2dm2cBK4Ap3/6jOmD8AFe5+u5kNAR5w94mNnVcJXURqNdR843vj8ilQ841jtKjk4u5bga3B+/vMbC3QH/iozrBTgTuCY9aZWb6Z9Xb3L1ocvYgkvbTUFCYP68vkYX2PNt94YnkpjywtYcIpvZg2Xs03ItGkD0XNLB9YCJzu7nvr/Px3QKa7/8zMzgaWAKPdfWW9508HpgPk5eWNLC0tbfkMRCQpbd9bxePLN/LEslI136gjKrtczCwLeAv4P+7+XL1jXYD7gDOBNcAQ4Afuvrqh86nkIiKRCNV849rReUwZk0+fru2v+UaLE7qZpQN/A/7p7veEGWvABmB43VV8fUroItIUoZpvXDKsL9PGt6/mGy2qoQcT9MPA2oaSuZl1A/a7+yHgB8DCxpK5iEhT1W++8cjSEv767iZeXK3mG7Ui2eUyHnibQCmlJvjjW4E8AHefZWZjCWxtPELgw9J/cfcvGzuvVugi0lKhmm9MGTuIa87Ko3vn5Gy+oW+KikhSa0/NN5TQRaTdSPbmG0roItLuhGq+8f2ifL6V4M03lNBFpN1KtuYbSugi0u4lS/MNXW1RRNq9cM03po4r4OtnJHbzDa3QRaTdSsTmGyq5iIg0IpGab6jkIiLSiLrNNz7bUcG8xYnZfEMrdBGRENpq8w2VXEREmqmtNd9QQhcRiYLa5hsvfbCF6hqPS/MNJXQRkSiKZ/MNJXQRkRiIR/MNJXQRkRhyd5Zv2MWcRbFvvqFtiyIiMWRmjDkhhzEnxLf5hlboIiIxUHGwmmdWbGJulJtvqOQiIhInNTXO/I8DzTcWrw8037j5wlP44TknNOt8KrmIiMRJSooxcWhvJg7tzbpte5m3uISBPTrG5HdF0iR6IIF+oX0I9BSd7e731RvTFXicQJ/RNOBud58b/XBFRBLXkD5duPNbw2N2/khW6NXAze7+npllAyvN7DV3/6jOmH8DPnL3b5hZT+BjM3vC3Q/FImgRETle2I9c3X2ru78XvL8PWAv0rz8MyLbAV6WygF0EXghERKSVNGkPjZnlA2cCy+sduh8YCmwB1gA3uXtNiOdPN7MVZrZix44dzYtYRERCijihm1kW8CzwU3ffW+/wxcAqoB9QCNxvZl3qn8PdZ7v7KHcf1bNnz2YHLSIix4sooZtZOoFk/oS7PxdiyFTgOQ9YD2wAhkQvTBERCSdsQg/WxR8G1rr7PQ0M2whMDI7vDZwCfB6tIEVEJLxIdrkUAVOANWa2KvizWwlsUcTdZwG/AeaZ2RrAgBnuvjP64YqISEPCJnR3X0QgSTc2ZgtwUbSCEhGRpovtlWJERKTVKKGLiCQJJXQRkSShhC4ikiSU0EVEkoQSuohIklBCFxFJEkroIiJJQgldRCRJKKGLiCQJJXQRkSShhC4ikiSU0EVEkoQSuohIklBCFxFJEkroIiJJQgldRCRJKKGLiCSJSJpEDzSz+Wa21sw+NLObQoz5uZmtCt6KzeyImfWITcgiIhJKJCv0auBmdx8KjAH+zcxOrTvA3f/g7oXuXgjMBN5y911Rj1ZERBoUNqG7+1Z3fy94fx+wFujfyFOuAZ6MTngiIhKpJtXQzSwfOBNY3sDxTsAk4NkGjk83sxVmtmLHjh1NDFVERBoTcUI3sywCifqn7r63gWHfABY3VG5x99nuPsrdR/Xs2bPp0YqISIMiSuhmlk4gmT/h7s81MvS7qNwiIhIXkexyMeBhYK2739PIuK7AucAL0QtPREQilRbBmCJgCrDGzFYFf3YrkAfg7rOCP7sSeNXdK6MdpIiIhBc2obv7IsAiGDcPmNfykEREpDn0TVERkSQRScmlbfn4H/DSv0PnntA5N/hv/ft1HnfoHO+IRURaReIl9KxecPIkqNwJlTvgyxWB+4f2hR6f3gk65TaS/Ovc75QDaR1adz4iIlGSeAm9/4jArb7DB75K8rX/7q/3uGIbfFEcuH/kUOjzZ3ark+gbWPXX3jK7QYqqViLSNiReQm9IekfoNjBwC8cdDu6t8wKw49jEX/vvzvVQuhT2lwN+/HkstU6Sr7fSb6j8Y2E/XxYRaZbkSehNYQaZXQO3nMHhx9ccgf27Gkj+de7vXhm4f7CBL9KmdWyk5FPvHUGnXJV/RKRJ2mdCb6qUVMjqGbhF4nDV8eWe+sk/ovJP1/Af+tYm/47dVf4RaeeU0GMhPRO6DgjcwnGHg/saXvXX3iIp/xwt9US4+0flH5GkooQeb2aQ2SVwa2r5J+S7gOD9Le+FKf9khkn+Kv+IJBol9EQTlfJP/d0/2+GLj4Lln4Ohz1Nb/jluC2iIdwEq/4jEhRJ6sotm+af2hWHX57BpeaD84zXHn8dSgom/kQ99jyn/ZKn8IxIFSujyleaUfw58GX73z5b3g+WfPaHPE678c8y7glxIy4juvEWShBK6NF9K6lcrb4aGH199METZp/7unwjKPxldQ+//P+4dQW35JzWq0xZpq5TQpfWkZUDX/oFbOO5wqKLx5B9x+acpu39U/pHEpYQubZMZZGQHbj1OCD/+mPJPI+8CtqxqvPyTmhF58lf5R9oYJXRJDseUfyIQqvwTahvojnWBMlDE5Z9GXghU/pEYU0KX9imq5Z/a3T8bYNM7gReGSMs/x+0GqnM/I1vlH2kSJXSRcJpc/qkJs/tnR6Dmv3V14H5Vc8o/uce/MKRnRnfeknDCJnQzGwg8CvQBaoDZ7n5fiHHnAfcC6cBOdz83moGKJIyUFOicE7gxJPz46oOBBN/Y1s/a8k/lDqiuCn2ejC7hv/lb+46gUw+Vf5JQJCv0auBmd3/PzLKBlWb2mrt/VDvAzLoBDwKT3H2jmfWKTbgiSSgtA7r0C9zCcYdDlRHs/glT/sEa2P2j8k8ii6RJ9FZga/D+PjNbC/QHPqoz7FrgOXffGBy3PQaxiogZZGQFbj0Kwo+vW/5p7Aqg2z4IU/7pEPnuH5V/4qZJNXQzywfOBJbXO3QykG5mC4Bs4D53fzTE86cD0wHy8vKaEa6INMkx5Z8IVB+qk/h3QGUDpaAdn0Dl9vDln8Y+9D36TWCVf6Il4oRuZlnAs8BP3b3+JfzSgJHARKAjsNTMlrn7J3UHuftsYDbAqFGjQlwDVkTiKq1D9Mo/tS8Mu0thc7D3rx8JcaJQ5Z9Gvgmc0UXlnwZElNDNLJ1AMn/C3Z8LMaSMwAehlUClmS0EzgA+CTFWRJJBc8o/Vbsb3/1TWQ7b1gTLP7tDn6ex8k+odwTtqPwTyS4XAx4G1rr7PQ0MewG438zSgA7AaOCPUYtSRBJfSkqgvNKpB/Q8Jfz46kMR7v75JLj750Do83TIDr/75+i1f3pAauLu5o4k8iJgCrDGzFYFf3YrkAfg7rPcfa2Z/QP4gMDWxofcvTgG8YpIe5HWAbr0DdwiEcnun4jKPz3CJP+2W/6JZJfLIiBsxO7+B+AP0QhKRKTJOnQO3Lrnhx97tPzTSPKv3Anbihsv/6SkN+3aP+kdozjh4yXuewsRkeY6pvxzcvjxoco/+0OUgMo/hYpw5Z8cOOsHMO7G6M4JJXQRkfCiVv4Jvihk9YlNmDE5q4hIe9aU8k8UqZOviEiSUEIXEUkSSugiIklCCV1EJEkooYuIJAkldBGRJKGELiKSJJTQRUSShLnH57LkZrYDKG3m03OBnVEMJxFozu2D5tw+tGTOg9y9Z6gDcUvoLWFmK9x9VLzjaE2ac/ugObcPsZqzSi4iIklCCV1EJEkkakKfHe8A4kBzbh805/YhJnNOyBq6iIgcL1FX6CIiUo8SuohIkmjTCd3MJpnZx2a23sxuCXHczOxPweMfmNmIeMQZTRHM+brgXD8wsyVmdkY84oymcHOuM+4sMztiZle1ZnyxEMmczew8M1tlZh+a2VutHWO0RfB/u6uZvWRmq4NznhqPOKPFzOaY2XYzK27gePTzl7u3yRuQCnwGnAB0AFYDp9YbcwnwCoEm1mOA5fGOuxXmPA7oHrw/uT3Muc64N4GXgaviHXcr/J27AR8BecHHveIddyvM+VbgruD9nsAuoEO8Y2/BnM8BRgDFDRyPev5qyyv0s4H17v65ux8CngIurzfmcuBRD1gGdDOzCJv+tUlh5+zuS9z9y+DDZcCAVo4x2iL5OwPcCDwLbG/N4GIkkjlfCzzn7hsB3D3R5x3JnB3INjMDsggk9OrWDTN63H0hgTk0JOr5qy0n9P7ApjqPy4I/a+qYRNLU+fwLgVf4RBZ2zmbWH7gSmNWKccVSJH/nk4HuZrbAzFaa2Q2tFl1sRDLn+4GhwBZgDXCTu9e0TnhxEfX81ZabRFuIn9XfYxnJmEQS8XzM7HwCCX18TCOKvUjmfC8ww92PBBZvCS+SOacBI4GJQEdgqZktc/dPYh1cjEQy54uBVcAEYDDwmpm97e57YxxbvEQ9f7XlhF4GDKzzeACBV+6mjkkkEc3HzIYDDwGT3b28lWKLlUjmPAp4KpjMc4FLzKza3f+nVSKMvkj/b+9090qg0swWAmcAiZrQI5nzVOBODxSY15vZBmAI8E7rhNjqop6/2nLJ5V3gJDMrMLMOwHeBF+uNeRG4Ifhp8Rhgj7tvbe1AoyjsnM0sD3gOmJLAq7W6ws7Z3QvcPd/d84FngH9N4GQOkf3ffgH4mpmlmVknYDSwtpXjjKZI5ryRwDsSzKw3cArweatG2bqinr/a7Ard3avN7CfAPwl8Qj7H3T80sx8Hj88isOPhEmA9sJ/AK3zCinDOvwBygAeDK9ZqT+Ar1UU456QSyZzdfa2Z/QP4AKgBHnL3kNvfEkGEf+ffAPPMbA2BcsQMd0/Yy+qa2ZPAeUCumZUBvwTSIXb5S1/9FxFJEm255CIiIk2ghC4ikiSU0EVEkoQSuohIklBCFxFJEkroIiJJQgldRCRJ/H82T9wRLGbW7QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index = y_tokenizer.index_word\n",
    "reverse_source_word_index = x_tokenizer.index_word\n",
    "target_word_index = y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Models\n",
    "\n",
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n",
    "                      state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
    "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
    "decoder_hidden_state_input = Input(shape=(max_plot_len, latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,\n",
    "        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
    "                      decoder_state_input_h, decoder_state_input_c],\n",
    "                      [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "\n",
    "    # Encode the input as state vectors.\n",
    "    (e_out, e_h, e_c) = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        (output_tokens, h, c) = decoder_model.predict([target_seq]\n",
    "                + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if sampled_token != 'eostok':\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find the stop word.\n",
    "        if sampled_token == 'eostok' or len(decoded_sentence.split()) \\\n",
    "            >= max_overview_len - 1:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        (e_h, e_c) = (h, c)\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert sequence to summary\n",
    "def seq2summary(input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if i != 0 and i != target_word_index['sostok'] and i \\\n",
    "            != target_word_index['eostok']:\n",
    "            newString = newString + reverse_target_word_index[i] + ' '\n",
    "\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert sequence to text\n",
    "def seq2text(input_seq):\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if i != 0:\n",
    "            newString = newString + reverse_source_word_index[i] + ' '\n",
    "\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: after coming off successful comedy tour dolemite throws get together at his mansion the party is crashed by racist police officers and they find out that the sheriff wife is offering dolemite money for sexual services when the sheriff catches them red handed he shoots and kills his wife dolemite and his friends kidnap young man and decide to head to california to meet queen bee there they find out that the local mob boss joe kidnapped two of queen bee girls forcing her to close her business and work for him dolemite rescues queen bee her girls and teaches his enemies lesson all while being chased by the sheriff who has the murder of his own wife on dolemite \n",
      "Original summary: start comes to the rescue of queen whose nightclub is threatened by the mafia end \n",
      "Predicted summary:  start a the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "\n",
      "Review: phillip is middle aged new york city architect who is going through difficult mid life crisis after learning that his wife has been having an affair leaves new york city and moves to greek island with his teenage daughter miranda in he meets singer and they become lovers mysteriously he takes vow of celibacy after they move to the island living on the island is an eccentric who was previously its only resident phillip finally seems happy until one day twist of fate brings his wife her new lover his ex boss and son freddy to the island due to shipwreck \n",
      "Original summary: start a mid life crisis in philip to the where the successful architect his marriage and career in for on remote greek island where he hopes to meaning into his life trying the of his new girlfriend and teenage daughter end \n",
      "Predicted summary:  start a the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "\n",
      "Review: bo price keith down and out country singer has returned home for his brother funeral following military training accident while there he reunites with his true love angela preston miami news reporter who has also returned home for her brother funeral bo also meets their 16 year old daughter dixie leigh for the first time since bo walked away from angela while she was still pregnant dixie has never met him or his side of the family dixie has with alcohol but is able to break free with the help of her now sober father with her father musical blood running through her dixie closes the movie by singing song she wrote at the memorial for the fallen soldiers \n",
      "Original summary: start a country music star returns to his hometown where he with his childhood sweetheart and also meets his 16 year old daughter for the first time end \n",
      "Predicted summary:  start a the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "\n",
      "Review: after the success of her first novel hastily on the life and loves of author allison mackenzie who follows in the footsteps of her mother by having an affair with married man her publisher lewis the of their situations bond allison and her mother when she returns to her hometown following the publication of her first novel samuel castle she is forced to face the wrath of most of its residents who are by their barely disguised and the revelation of town secrets in the book despite that certain members of the community stood by the most notably seth the newspaper editor and his oldest friend dr matthew in fact whenever anyone came into dr office and about allison book he would them down and after harsh from him about some of the things that person had done he or she ever complain about allison novel after that however roberta carter member of the school board working in concert with the town attorney wife marion partridge makes it her mission to ban the book from the high school library she punishes allison by firing her stepfather michael rossi decision which she eventually to the anger of marion while at the same time trying to her son ted marriage to his snobbish bride boston blue blood named jennifer another union in trouble is that of allison mother constance who is shocked by her daughter but nonetheless stands by her and stepfather michael rossi the school principal and one of the novel betty anderson returns from new york after giving birth to roddy the child she had by rodney harrington and along with her co and roddy agnes moves to place so she can allow leslie roddy grandfather to know him cross who had been acquitted of murder in the previous novel was trying to make life for herself and her brother joey she is manager of the corner and is success in this book and allison had as friends and allison new york roommate stephanie wallace was also part of their circle \n",
      "Original summary: start more in the little town full of secrets when author to loose some end \n",
      "Predicted summary:  start a the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 4):\n",
    "    print ('Review:', seq2text(x_tr[i]))\n",
    "    print ('Original summary:', seq2summary(y_tr[i]))\n",
    "    print ('Predicted summary:', decode_sequence(x_tr[i].reshape(1,\n",
    "           max_plot_len)))\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Either add an attention layer or discuss it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we were using an encoder-decoder architecture for machine translation, where one RNN reads in a sentence and a different one outputs a sentence. But we know that the Bleu score comes down ater 30 or 40 word sentences.  The attention model helps this problem greatly. It works by ********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to put together a very simple neural network for a text summarization task, let's see how the current best models perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '4.2'></a>\n",
    "<h3 style = \"font-family:garamond; font-size:30px; background-color: white; color : royalblue; border-radius: 100px 100px; text-align:left\">T5 pre-trained</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers==2.9.0\n",
      "  Downloading transformers-2.9.0-py3-none-any.whl (635 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from transformers==2.9.0) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from transformers==2.9.0) (4.62.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from transformers==2.9.0) (3.3.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from transformers==2.9.0) (1.20.3)\n",
      "Collecting tokenizers==0.7.0\n",
      "  Downloading tokenizers-0.7.0.tar.gz (81 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from transformers==2.9.0) (2021.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==2.9.0) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from requests->transformers==2.9.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from requests->transformers==2.9.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from requests->transformers==2.9.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from requests->transformers==2.9.0) (2021.10.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.9.0) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.9.0) (8.0.3)\n",
      "Requirement already satisfied: six in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.9.0) (1.16.0)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\rotzn\\anaconda3\\python.exe' 'C:\\Users\\rotzn\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' build_wheel 'C:\\Users\\rotzn\\AppData\\Local\\Temp\\tmpig94a7nl'\n",
      "       cwd: C:\\Users\\rotzn\\AppData\\Local\\Temp\\pip-install-40n58drx\\tokenizers_0df8d5b66aef497a9b74b2c08523f732\n",
      "  Complete output (46 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.9\n",
      "  creating build\\lib.win-amd64-3.9\\tokenizers\n",
      "  copying tokenizers\\__init__.py -> build\\lib.win-amd64-3.9\\tokenizers\n",
      "  creating build\\lib.win-amd64-3.9\\tokenizers\\models\n",
      "  copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-3.9\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-3.9\\tokenizers\\decoders\n",
      "  copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-3.9\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-3.9\\tokenizers\\normalizers\n",
      "  copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-3.9\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-3.9\\tokenizers\\pre_tokenizers\n",
      "  copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-3.9\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-3.9\\tokenizers\\processors\n",
      "  copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-3.9\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-3.9\\tokenizers\\trainers\n",
      "  copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-3.9\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-3.9\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-3.9\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-3.9\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-3.9\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-3.9\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-3.9\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-3.9\\tokenizers\\implementations\n",
      "  copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-3.9\\tokenizers\n",
      "  copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-3.9\\tokenizers\\models\n",
      "  copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-3.9\\tokenizers\\decoders\n",
      "  copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-3.9\\tokenizers\\normalizers\n",
      "  copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-3.9\\tokenizers\\pre_tokenizers\n",
      "  copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-3.9\\tokenizers\\processors\n",
      "  copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-3.9\\tokenizers\\trainers\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pytorch_lightning==0.7.5\n",
      "  Downloading pytorch_lightning-0.7.5-py3-none-any.whl (233 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from pytorch_lightning==0.7.5) (4.62.3)\n",
      "Collecting tensorboard>=1.14\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: future>=0.17.1 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from pytorch_lightning==0.7.5) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.16.4 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from pytorch_lightning==0.7.5) (1.20.3)\n",
      "Collecting torch>=1.1\n",
      "  Downloading torch-1.10.0-cp39-cp39-win_amd64.whl (226.5 MB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Downloading protobuf-3.19.1-cp39-cp39-win_amd64.whl (895 kB)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (2.26.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (2.0.2)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.42.0-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (0.37.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from tensorboard>=1.14->pytorch_lightning==0.7.5) (58.0.4)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from torch>=1.1->pytorch_lightning==0.7.5) (3.10.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from tqdm>=4.41.0->pytorch_lightning==0.7.5) (0.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from absl-py>=0.4->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.16.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (4.8.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning==0.7.5) (2.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\rotzn\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning==0.7.5) (3.6.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, torch, tensorboard, pytorch-lightning\n",
      "Successfully installed absl-py-1.0.0 cachetools-4.2.4 google-auth-2.3.3 google-auth-oauthlib-0.4.6 grpcio-1.42.0 markdown-3.3.6 oauthlib-3.1.1 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pytorch-lightning-0.7.5 requests-oauthlib-1.3.0 rsa-4.8 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 torch-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==2.9.0 \n",
    "!pip install pytorch_lightning==0.7.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = single_samp_plot.strip().replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15464/1535408083.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.encode(processed_text, return_tensor=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(tokenized_text,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=30,\n",
    "                                    max_length=100,\n",
    "                                    early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\n\\nSummarized text: \\n\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9344/3368350506.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_10\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPlot\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mt5_prepared_Text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"summarize: \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt5_prepared_Text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     summary_ids = model.generate(tokenized_text,\n\u001b[0;32m      7\u001b[0m                                 \u001b[0mnum_beams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "T5_generated_overview = []\n",
    "device = torch.device('cpu')\n",
    "for i in df_10.Plot:\n",
    "    try:\n",
    "        t5_prepared_Text = \"summarize: \"+ i\n",
    "        tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "        summary_ids = model.generate(tokenized_text,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=40,\n",
    "                                    max_length=400,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "        output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        T5_generated_overview.append(output)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T5_generated_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80s[\"generated_overview\"] = generated_overview\n",
    "df_80s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS HOW TO GET THE COSINE DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80s[\"overview_tokens\"] = df_80s[\"Overview\"].apply(lambda x: x.split(\" \"))\n",
    "df_80s[\"generated_overview_tokens\"] = df_80s[\"generated_overview\"].apply(lambda x: x.split(\" \"))\n",
    "df_80s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80s[\"cos_sim\"] = df_80s.apply(lambda x: cosine_distance_wordembedding_method(x.generated_overview_tokens, x.overview_tokens), axis=1)\n",
    "df_80s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80s[\"cos_sim\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_80s.iloc[0][\"Overview\"],\"\\n\\n\", df_80s.iloc[0][\"generated_overview\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_80s.iloc[1][\"Overview\"],\"\\n\\n\", df_80s.iloc[1][\"generated_overview\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_80s.iloc[2][\"Overview\"],\"\\n\\n\", df_80s.iloc[2][\"generated_overview\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80s = df_80s[['Release Year', 'Title', 'Plot', 'Overview', 'cleaned_plot', 'cleaned_overview', 'overview_tokes', 'generated_overview', 'cos_sim']]\n",
    "df_80s = df_80s.rename(columns={'generated_overview': 'Abs_generated_overview', 'cos_sim': 'Abs_cos_sim'})\n",
    "df_80s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '4.3'></a>\n",
    "<h3 style = \"font-family:garamond; font-size:30px; background-color: white; color : royalblue; border-radius: 100px 100px; text-align:left\">BERT (pre-trained)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to hardware constraints, we will just be using the pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "'''os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall use single_sample_plot again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_text = summarizer(single_samp_plot, max_length=120, min_length=15, do_sample=False)[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Pytorch Bart Overview:   In 1944, American forces were closing in on the Japanese-occupied Philippines . The Japanese held around 500 American prisoners who had survived the Bataan Death March in a notorious POW camp at Cabanatuan . The film chronicles the efforts of the Rangers, Alamo Scouts from the Sixth Army and Filipino guerrillas .\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nPytorch Bart Overview: \", summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "Your max_length is set to 120, but you input_length is only 115. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    }
   ],
   "source": [
    "bart_generated_overview = []\n",
    "for i in df_10['Plot']:\n",
    "    summarizer = pipeline('summarization')\n",
    "    summary_text = summarizer(i, max_length=120, min_length=15, do_sample=False)[0]['summary_text']\n",
    "    bart_generated_overview.append(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '5'></a>\n",
    "<h1 style = \"font-family: garamond; font-size: 35px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :royalblue; border-radius: 100px 100px; text-align:left \" >Results</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '5.1'></a>\n",
    "<h1 style = \"font-family: garamond; font-size: 30px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :royalblue; border-radius: 100px 100px; text-align:left \" >Subjective measures</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Let Him Have It \n",
      "\n",
      "Legit Overview:  In 1950s England, slow-witted Derek Bentley falls in with a group of petty criminals led by Chris Craig, a teenager with a fondness for American gangster films. Chris and Derek's friendship leads to their involvement in the true case which would forever shake England's belief in capital punishment. \n",
      "\n",
      "spacy Overview:  Derek Bentley (Eccleston) is an illiterate, epileptic young adult with developmental disabilities who falls into a gang led by a younger teenager named Christopher Craig (Reynolds). Meanwhile, Bentley is sentenced to death under the English common law principle of joint enterprise, on the basis that his statement to Craig was an instigation to begin shooting. Bentley, who by this time has already been arrested, shouts \"Let him have it, Chris\" – whether he means the phrase literally (\"Let him have the gun\") or figuratively (\"Open fire!\") is unclear. \n",
      "\n",
      "gensim overview:  nothing yet \n",
      "\n",
      "luhn overview:  During the course of the robbery of a warehouse in Croydon, in which Bentley is encouraged to participate by Craig, the two become trapped by the police. Bentley, who by this time has already been arrested, shouts \"Let him have it, Chris\" – whether he means the phrase literally (\"Let him have the gun\") or figuratively (\"Open fire!\") Because he is a minor, Craig is given a prison sentence for the crime. Bentley's family begins an effort for clemency which reaches Parliament. However, the Home Secretary (who has the power to commute the death sentence) declines to intervene. Despite his family's efforts and some public support, Bentley is executed in 1953 within a month of being convicted, before Parliament takes any official action.  \n",
      "\n",
      "LSTM overview:  nothing yet \n",
      "\n",
      "T5 overview:  nothing yet \n",
      "\n",
      "bart overview:   Derek Bentley (Eccleston) is an illiterate, epileptic young adult with developmental disabilities . He is sentenced to death under the English common law principle of joint enterprise . Bentley is executed in 1953 within a month of being convicted, before Parliament takes any official action .\n"
     ]
    }
   ],
   "source": [
    "print('Title: ', df_10.iloc[0]['Title'],\n",
    "      '\\n\\nLegit Overview: ',  df_10.iloc[0][\"Overview\"],\n",
    "      \"\\n\\nspacy Overview: \", spacy_generated_overview[0],\n",
    "      \"\\n\\ngensim overview: \", 'nothing yet', #gensim_generated_overview\n",
    "      \"\\n\\nluhn overview: \", sumy_luhn_generated_overview[0],\n",
    "      \"\\n\\nLSTM overview: \", 'nothing yet', #LSTM_generated_overview\n",
    "      \"\\n\\nT5 overview: \", 'nothing yet', #T5_generated_overview\n",
    "      \"\\n\\nbart overview: \", bart_generated_overview[0]\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Title: ', df_10.iloc[1]['Title'],\n",
    "      '\\n\\nLegit Overview: ',  df_10.iloc[1][\"Overview\"],\n",
    "      \"\\n\\nspacy Overview: \", spacy_generated_overview[1],\n",
    "      \"\\n\\ngensim overview: \", 'nothing yet', #gensim_generated_overview\n",
    "      \"\\n\\nluhn overview: \", sumy_luhn_generated_overview[1],\n",
    "      \"\\n\\nLSTM overview: \", 'nothing yet', #LSTM_generated_overview\n",
    "      \"\\n\\nT5 overview: \", 'nothing yet', #T5_generated_overview\n",
    "      \"\\n\\nbart overview: \", bart_generated_overview[1]\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '5.2'></a>\n",
    "<h1 style = \"font-family: garamond; font-size: 35px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :royalblue; border-radius: 100px 100px; text-align:left \" >Objective measures</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../global_vectors_for_word_rep/glove.6B.200d.txt\"\n",
    "final = glove2word2vec(path ,\"./glove.6B.200d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "might have to change path of below to the correct path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = gensim.models.KeyedVectors.load_word2vec_format(\"./glove.6B.200d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance_wordembedding_method(sent1, sent2):\n",
    "    vector_1 = np.mean([glove[word] for word in sent1 if word in glove],axis=0)\n",
    "    vector_2 = np.mean([glove[word] for word in sent2 if word in glove],axis=0)\n",
    "    cosine = scipy.spatial.distance.cosine(vector_1, vector_2)\n",
    "    return 1-cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80s[\"Ext_generated_overview_tokens\"] = df_80s[\"Ext_generated_overview\"].apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80s[\"Ext_cos_sim\"] = df_80s.apply(lambda x: cosine_distance_wordembedding_method(x.Ext_generated_overview_tokens, x.overview_tokens), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = df_80s.iloc[1]['Abs_generated_overview']\n",
    "extract = df_80s.iloc[1]['Ext_generated_overview']\n",
    "ref = df_80s.iloc[1]['Overview']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "score_abs = rouge.get_scores(abstract, ref)\n",
    "score_ext = rouge.get_scores(extract, ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Abstractive cos sim mean: ', df_80s[\"Abs_cos_sim\"].mean() )\n",
    "print('Extractive cos sim mean: ', df_80s[\"Ext_cos_sim\"].mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ext[0]['rouge-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(score_ext[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_plot = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in score_ext[0].items():\n",
    "    ext_list = []\n",
    "    for s, t in score_ext[0][k].items():\n",
    "        ext_list.append(t)\n",
    "    ext_series = pd.Series(ext_list)\n",
    "    ext_plot[k] = ext_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_plot = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in score_abs[0].items():\n",
    "    abs_list = []\n",
    "    for s, t in score_abs[0][k].items():\n",
    "        abs_list.append(t)\n",
    "    abs_series = pd.Series(abs_list)\n",
    "    abs_plot[k] = abs_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abs_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(abs_plot.T)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypoints = np.array([3, 8, 1, 10])\n",
    "\n",
    "plt.plot(ypoints, linestyle = 'dotted')\n",
    "plt.plot(ypoints + 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = '6'></a>\n",
    "<h1 style = \"font-family: garamond; font-size: 45px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :royalblue; border-radius: 100px 100px; text-align:center \" >Summary</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rewrite below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the extractive approach scores better on cosine similarity and on the rouge scores. This is to be expected because the extractive approach literally lifts full sentences from the original text. The real test is the human subjective test, and on that the winner goes to the abstractive method. I think the vast majority of people would agree with this. With the extractive approach, the sentences feel out of place or inserted at random and they do not flow together. With the abstractive approach, there is a flow to the sentences, even if the content sometimes doesn't make sense. I think for the average review, one would glean a more accurate summarization of the movie from the abstractive approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
